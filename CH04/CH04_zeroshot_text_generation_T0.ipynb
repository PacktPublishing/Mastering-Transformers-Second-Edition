{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iw1KlzcbheF"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
        "\n",
        "nlp=pipeline(\"summarization\",\n",
        "\n",
        "     model=model,\n",
        "\n",
        "     tokenizer=tokenizer)\n",
        "\n",
        "text='''\n",
        "\n",
        "We order two different types of jewelry from this\n",
        "\n",
        "company the other jewelry we order is perfect.\n",
        "\n",
        "However with this jewelry I have a few things I\n",
        "\n",
        "don't like. The little Stone comes out of these\n",
        "\n",
        "and customers are complaining and bringing them\n",
        "\n",
        "back and we are having to put new jewelry in their\n",
        "\n",
        "holes. You cannot sterilize these in an autoclave\n",
        "\n",
        "...[truncated]'''\n",
        "\n",
        "q=nlp(text)\n",
        "\n",
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=0, width=100)\n",
        "\n",
        "pp.pprint(q[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XF1GtrxHbl4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}